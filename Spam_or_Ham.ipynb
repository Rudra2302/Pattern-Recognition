{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the \"20050311_spam_2\" and the \"20021010_easy_ham\" folders from Spamassassin before running this code. Make sure directory names match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset into an array and store their labels\n",
    "# First, the spam emails, all labelled 1\n",
    "dataset_dir = \"20050311_spam_2\"\n",
    "emails=[]\n",
    "labels=[]\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"latin1\") as file:\n",
    "                email_content = file.read()\n",
    "                emails=np.append(emails,email_content)\n",
    "                labels=np.append(labels,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next the ham emails, all labelled 0\n",
    "dataset_dir = \"20021010_easy_ham\"\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"latin1\") as file:\n",
    "                email_content = file.read()\n",
    "                emails=np.append(emails,email_content)\n",
    "                labels=np.append(labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove unwanted html tags from emails\n",
    "import re\n",
    "\n",
    "def remove_html_tags(email):\n",
    "    clean_email = re.sub(r'<[^>]+>', '', email)\n",
    "    return clean_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a word dictionary out of all the emails\n",
    "# This dictionary will contains all the words used and their frequencies\n",
    "from collections import Counter\n",
    "\n",
    "def build_dictionary(emails):\n",
    "    word_counter = Counter()\n",
    "    for email in emails:\n",
    "        email = remove_html_tags(email)\n",
    "        words = email.split()\n",
    "        word_counter.update(words)\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort the top 10000 most frequently used words into an array\n",
    "def sort_dictionary(emails):\n",
    "    word_counter = build_dictionary(emails)\n",
    "    sorted_dict = dict(sorted(word_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_dict = dict(list(sorted_dict.items())[0:10000])\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the email into a datapoint\n",
    "def make_data(email, sorted_dict):\n",
    "    words = email.split()\n",
    "    X = np.zeros(10000)\n",
    "    i = 0\n",
    "    for key in sorted_dict:\n",
    "        if key in words:\n",
    "            X[i] = 1\n",
    "        i = i+1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform our whole dataset into binary arrays\n",
    "def data(emails):\n",
    "    sorted_dict = sort_dictionary(emails)\n",
    "    X=np.empty((len(emails),10000))\n",
    "    for i in range(len(emails)):\n",
    "        X[i] = make_data(emails[i],sorted_dict)\n",
    "    return X\n",
    "\n",
    "X = data(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM modelling function\n",
    "# Using Library function SVC()\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm(X_train, Y_train, X_test, Y_test, c):\n",
    "    svm_classifier = SVC(kernel='linear', C=c) # Give c as hyper parameter\n",
    "    svm_classifier.fit(X_train, Y_train)       # Fit a classifier model\n",
    "    Y_pred = svm_classifier.predict(X_test)    # Prdict labels on the test data\n",
    "    error = 0\n",
    "    for i in range(Y_pred.shape[0]):\n",
    "        if Y_test[i] != Y_pred[i] :\n",
    "            error += 1\n",
    "    accuracy = 1 - error/Y_test.shape[0]       # Calculate accuracy\n",
    "    return Y_pred, accuracy, svm_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split dataset into test and training data and running SVM\n",
    "def run_svm(X,labels):\n",
    "    ind = np.random.permutation(X.shape[0])  # Randomize the dataset\n",
    "    X_new = X[ind]\n",
    "    Y_new = [labels[i] for i in ind]\n",
    "\n",
    "    index = int (0.8*X.shape[0])             # Do an 80:20 split on the dataset\n",
    "    X_train = X_new[:index, :]\n",
    "    X_test = X_new[index:, :]\n",
    "    Y_train = np.array(Y_new[:index])\n",
    "    Y_test = np.array(Y_new[index:])\n",
    "\n",
    "    c=1e9 # Change depending on problem\n",
    "    Y_pred, accuracy, svm_classifier = svm(X_train, Y_train, X_test, Y_test, c)\n",
    "    return Y_pred, accuracy, svm_classifier\n",
    "\n",
    "Y_pred_svm , accuracy_svm, svm_classifier = run_svm(X,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do the Naive-Bayes' Algorithm\n",
    "def naive_bayes(X_train, Y_train, X_test, Y_test):\n",
    "    n_spam = np.sum(Y_train)             # Calculate number of spam mails\n",
    "    n_ham = Y_train.shape[0]-n_spam\n",
    "    p_hat = n_spam/(n_ham+n_spam)\n",
    "    p = np.zeros((2,10000))\n",
    "    for j in range(10000):\n",
    "        for i in range(X_train.shape[0]):\n",
    "            p[int(Y_train[i])][j] += X_train[i][j]    # Update p values\n",
    "    p[0] = p[0]/n_ham\n",
    "    p[1] = p[1]/n_spam\n",
    "\n",
    "    Y_pred=np.zeros(Y_test.shape[0])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        p0=1-p_hat\n",
    "        p1=p_hat\n",
    "        for j in range(10000):\n",
    "            p0 = p0*pow(p[0][j],X_test[i][j])*pow(1-p[0][j],1-X_test[i][j])    # P(y=0|x)\n",
    "            p1 = p1*pow(p[1][j],X_test[i][j])*pow(1-p[1][j],1-X_test[i][j])    # P(y=1|x)\n",
    "        if p1 > p0:\n",
    "            Y_pred[i] = 1  # Predict based on probability values\n",
    "    \n",
    "    error = 0\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        if Y_test[i] != Y_pred[i]:\n",
    "            error += 1\n",
    "    accuracy = 1 - error/Y_test.shape[0]    # Calculate accuracy\n",
    "    \n",
    "    return p, p_hat, Y_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datset and feed it to the Naive-Bayes' function\n",
    "def run_naive_bayes(X,labels):\n",
    "    ind = np.random.permutation(X.shape[0])\n",
    "    X_new = X[ind]\n",
    "    Y_new = [labels[i] for i in ind]\n",
    "\n",
    "    index = int (0.8*X.shape[0])\n",
    "    X_train = X_new[:index, :]\n",
    "    X_test = X_new[index:, :]\n",
    "    Y_train = np.array(Y_new[:index])\n",
    "    Y_test = np.array(Y_new[index:])\n",
    "\n",
    "    p, p_hat, Y_pred, accuracy=naive_bayes(X_train, Y_train, X_test, Y_test)\n",
    "    return p, p_hat, Y_pred, accuracy\n",
    "\n",
    "p, p_hat, Y_pred_NB, accuracy_NB = run_naive_bayes(X,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out logistic regression\n",
    "def logistic_gd(X_train, Y_train, X_test, Y_test):\n",
    "    w = np.zeros(10000)  # Initialize w\n",
    "    eta = 1e-6           # Step size\n",
    "    error = 1\n",
    "    threshold  = 0.0005  # Set a threshold\n",
    "\n",
    "    while error>threshold:\n",
    "        grad = 0\n",
    "        for i in range(X_train.shape[0]):\n",
    "            grad = grad + X_train[i]*(Y_train[i]-(1/(1+np.exp(-(w.T @ X_train[i]))))) # Find the gradient\n",
    "        grad = grad*eta\n",
    "        error = np.linalg.norm(grad)   # Update error value\n",
    "        w = w + grad   # Update w value\n",
    "    \n",
    "    Y_pred = np.zeros(Y_test.shape[0])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        pred = 1/(1+np.exp(-(w.T @ X_test[i])))   # Find sigmoid function value\n",
    "        if pred > 0.5:\n",
    "            Y_pred[i] = 1  # Predict based on sigmoid function value\n",
    "\n",
    "    err = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if Y_pred[i] != Y_test[i]:\n",
    "            err = err+1\n",
    "    accuracy = 1 - err/Y_test.shape[0]   # Calculate accuracy\n",
    "    return w, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset and feed to the logistic regressor function\n",
    "def run_logistic(X,labels):\n",
    "    ind = np.random.permutation(X.shape[0])\n",
    "    X_new = X[ind]\n",
    "    Y_new = [labels[i] for i in ind]\n",
    "\n",
    "    index = int (0.8*X.shape[0])\n",
    "    X_train = X_new[:index, :]\n",
    "    X_test = X_new[index:, :]\n",
    "    Y_train = np.array(Y_new[:index])\n",
    "    Y_test = np.array(Y_new[index:])\n",
    "\n",
    "    w,accuracy=logistic_gd(X_train, Y_train, X_test, Y_test)\n",
    "    return w,accuracy\n",
    "\n",
    "w_logistic, accuracy_logistic = run_logistic(X,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run on test emails\n",
    "def run_test_emails():\n",
    "    dataset_dir = \"test\"  # Name the directory \"test\"\n",
    "    test_emails=[]\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_dir):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, \"r\", encoding=\"latin1\") as file:\n",
    "                    email_content = file.read()\n",
    "                    test_emails=np.append(test_emails,email_content)\n",
    "\n",
    "    test_X = data(test_emails)\n",
    "    Y_pred_test_svm = svm_classifier.predict(test_X)  # SVM predictor results\n",
    "\n",
    "\n",
    "    Y_pred_test_NB = np.zeros(test_X.shape[0])   # Naive-Bayes' predictor\n",
    "    for i in range(test_X.shape[0]):\n",
    "        p0=1-p_hat\n",
    "        p1=p_hat\n",
    "        for j in range(10000):\n",
    "            p0 = p0*pow(p[0][j],test_X[i][j])*pow(1-p[0][j],1-test_X[i][j]) \n",
    "            p1 = p1*pow(p[1][j],test_X[i][j])*pow(1-p[1][j],1-test_X[i][j]) \n",
    "        if p1 > p0:\n",
    "            Y_pred_test_NB[i] = 1 \n",
    "\n",
    "    Y_pred_test_logistic = np.zeros(test_X.shape[0])  # Logistic Regression predictor\n",
    "    for i in range(test_X.shape[0]):\n",
    "        pred = 1/(1+np.exp(-(w_logistic.T @ test_X[i]))) \n",
    "        if pred > 0.5:\n",
    "            Y_pred_test_logistic[i] = 1\n",
    "\n",
    "    return Y_pred_test_svm, Y_pred_test_NB, Y_pred_test_logistic # Return all 3 prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading the test data, run all codes above and finally print the solution by running this snippet\n",
    "Y_pred_test_svm, Y_pred_test_NB, Y_pred_test_logistic = run_test_emails()\n",
    "print(Y_pred_test_svm)\n",
    "print(Y_pred_test_NB)\n",
    "print(Y_pred_test_logistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
